{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2097406,"sourceType":"datasetVersion","datasetId":1257880}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torchvision.transforms import v2\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom PIL import Image\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport random\nimport shutil","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:41:02.894163Z","iopub.execute_input":"2024-02-03T17:41:02.895002Z","iopub.status.idle":"2024-02-03T17:41:06.263405Z","shell.execute_reply.started":"2024-02-03T17:41:02.894965Z","shell.execute_reply":"2024-02-03T17:41:06.262495Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n  warnings.warn(_BETA_TRANSFORMS_WARNING)\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n  warnings.warn(_BETA_TRANSFORMS_WARNING)\n","output_type":"stream"}]},{"cell_type":"code","source":"clahe = cv2.createCLAHE(clipLimit=0.03, tileGridSize=(8, 8))","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:41:06.264789Z","iopub.execute_input":"2024-02-03T17:41:06.265727Z","iopub.status.idle":"2024-02-03T17:41:06.270976Z","shell.execute_reply.started":"2024-02-03T17:41:06.265674Z","shell.execute_reply":"2024-02-03T17:41:06.270139Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_dict = {}\nval_dict = {}\ntest_dict = {}\ntrain_path = \"/kaggle/input/knee-osteoarthritis-dataset-with-severity/train/\"\nval_path = \"/kaggle/input/knee-osteoarthritis-dataset-with-severity/val/\"\ntest_path = \"/kaggle/input/knee-osteoarthritis-dataset-with-severity/test/\"","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:41:06.273084Z","iopub.execute_input":"2024-02-03T17:41:06.273824Z","iopub.status.idle":"2024-02-03T17:41:06.280358Z","shell.execute_reply.started":"2024-02-03T17:41:06.273786Z","shell.execute_reply":"2024-02-03T17:41:06.279513Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"for label in os.listdir(train_path):\n    train_dict[int(label)] = os.listdir(train_path + label)\nfor label in os.listdir(test_path):\n    test_dict[int(label)] = os.listdir(test_path + label)\nfor label in os.listdir(val_path):\n    val_dict[int(label)] = os.listdir(val_path + label)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:41:06.281402Z","iopub.execute_input":"2024-02-03T17:41:06.281653Z","iopub.status.idle":"2024-02-03T17:41:06.301960Z","shell.execute_reply.started":"2024-02-03T17:41:06.281631Z","shell.execute_reply":"2024-02-03T17:41:06.301085Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_images = {}\nval_images = {}\ntest_images = {}","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:41:06.303051Z","iopub.execute_input":"2024-02-03T17:41:06.303365Z","iopub.status.idle":"2024-02-03T17:41:06.308083Z","shell.execute_reply.started":"2024-02-03T17:41:06.303334Z","shell.execute_reply":"2024-02-03T17:41:06.307170Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"for keys in train_dict.keys():  \n    train_normal_images = [np.array(Image.open(train_path + str(keys) + \"/\" + images)) for images in train_dict[keys]]\n    test_normal_images = [np.array(Image.open(test_path + str(keys) + \"/\" + images)) for images in test_dict[keys]]\n    val_normal_images = [np.array(Image.open(val_path + str(keys) + \"/\" + images)) for images in val_dict[keys]]\n    train_images[keys] = train_normal_images\n    test_images[keys] = test_normal_images\n    val_images[keys] = val_normal_images","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:41:06.309134Z","iopub.execute_input":"2024-02-03T17:41:06.309398Z","iopub.status.idle":"2024-02-03T17:41:51.767559Z","shell.execute_reply.started":"2024-02-03T17:41:06.309374Z","shell.execute_reply":"2024-02-03T17:41:51.766594Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"trlabel0 = [0 for i in range(len(train_images[0]))]\ntrlabel1 = [1 for i in range(len(train_images[1]))]\ntrlabel2 = [2 for i in range(len(train_images[2]))]\ntrlabel3 = [3 for i in range(len(train_images[3]))]\ntrlabel4 = [4 for i in range(len(train_images[4]))]","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:41:51.768834Z","iopub.execute_input":"2024-02-03T17:41:51.769453Z","iopub.status.idle":"2024-02-03T17:41:51.777885Z","shell.execute_reply.started":"2024-02-03T17:41:51.769419Z","shell.execute_reply":"2024-02-03T17:41:51.776975Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tslabel0 = [0 for i in range(len(test_images[0]))]\ntslabel1 = [1 for i in range(len(test_images[1]))]\ntslabel2 = [2 for i in range(len(test_images[2]))]\ntslabel3 = [3 for i in range(len(test_images[3]))]\ntslabel4 = [4 for i in range(len(test_images[4]))]","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:41:51.779075Z","iopub.execute_input":"2024-02-03T17:41:51.779331Z","iopub.status.idle":"2024-02-03T17:41:51.786699Z","shell.execute_reply.started":"2024-02-03T17:41:51.779310Z","shell.execute_reply":"2024-02-03T17:41:51.785878Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"vlabel0 = [0 for i in range(len(val_images[0]))]\nvlabel1 = [1 for i in range(len(val_images[1]))]\nvlabel2 = [2 for i in range(len(val_images[2]))]\nvlabel3 = [3 for i in range(len(val_images[3]))]\nvlabel4 = [4 for i in range(len(val_images[4]))]","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:41:51.788178Z","iopub.execute_input":"2024-02-03T17:41:51.788569Z","iopub.status.idle":"2024-02-03T17:41:51.797082Z","shell.execute_reply.started":"2024-02-03T17:41:51.788508Z","shell.execute_reply":"2024-02-03T17:41:51.796124Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"training_image  = torch.tensor(torch.cat((torch.tensor(train_images[0]),torch.tensor(train_images[1]),torch.tensor(train_images[2]),torch.tensor(train_images[3]),torch.tensor(train_images[4])),0),dtype = torch.float32)\ntraining_labels = torch.tensor(torch.cat((torch.tensor(trlabel0),torch.tensor(trlabel1),torch.tensor(trlabel2),torch.tensor(trlabel3),torch.tensor(trlabel4)),0))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:41:51.801411Z","iopub.execute_input":"2024-02-03T17:41:51.801672Z","iopub.status.idle":"2024-02-03T17:43:11.653210Z","shell.execute_reply.started":"2024-02-03T17:41:51.801650Z","shell.execute_reply":"2024-02-03T17:43:11.652195Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"training_image = training_image.view(training_image.shape[0],1,224,224)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:43:11.654390Z","iopub.execute_input":"2024-02-03T17:43:11.654691Z","iopub.status.idle":"2024-02-03T17:43:11.664576Z","shell.execute_reply.started":"2024-02-03T17:43:11.654666Z","shell.execute_reply":"2024-02-03T17:43:11.663640Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"testing_image  = torch.tensor(torch.cat((torch.tensor(test_images[0]),torch.tensor(test_images[1]),torch.tensor(test_images[2]),torch.tensor(test_images[3]),torch.tensor(test_images[4])),0),dtype = torch.float32)\ntesting_labels = torch.tensor(torch.cat((torch.tensor(tslabel0),torch.tensor(tslabel1),torch.tensor(tslabel2),torch.tensor(tslabel3),torch.tensor(tslabel4)),0))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:43:11.665586Z","iopub.execute_input":"2024-02-03T17:43:11.665922Z","iopub.status.idle":"2024-02-03T17:43:34.413535Z","shell.execute_reply.started":"2024-02-03T17:43:11.665892Z","shell.execute_reply":"2024-02-03T17:43:34.412697Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"testing_image = testing_image.view(testing_image.shape[0],1,224,224)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:43:34.414761Z","iopub.execute_input":"2024-02-03T17:43:34.415112Z","iopub.status.idle":"2024-02-03T17:43:34.420447Z","shell.execute_reply.started":"2024-02-03T17:43:34.415082Z","shell.execute_reply":"2024-02-03T17:43:34.419563Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"val_image  = torch.tensor(torch.cat((torch.tensor(val_images[0]),torch.tensor(val_images[1]),torch.tensor(val_images[2]),torch.tensor(val_images[3]),torch.tensor(val_images[4])),0),dtype = torch.float32)\nval_labels = torch.tensor(torch.cat((torch.tensor(vlabel0),torch.tensor(vlabel1),torch.tensor(vlabel2),torch.tensor(vlabel3),torch.tensor(vlabel4)),0))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:43:34.421531Z","iopub.execute_input":"2024-02-03T17:43:34.421873Z","iopub.status.idle":"2024-02-03T17:43:45.718670Z","shell.execute_reply.started":"2024-02-03T17:43:34.421840Z","shell.execute_reply":"2024-02-03T17:43:45.717732Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"val_image = val_image.view(val_image.shape[0],1,224,224)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:43:45.719913Z","iopub.execute_input":"2024-02-03T17:43:45.720195Z","iopub.status.idle":"2024-02-03T17:43:45.725330Z","shell.execute_reply.started":"2024-02-03T17:43:45.720172Z","shell.execute_reply":"2024-02-03T17:43:45.724065Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_dataset = TensorDataset(training_image,training_labels)\nval_dataset = TensorDataset(val_image,val_labels)\ntest_dataset = TensorDataset(testing_image,testing_labels)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:43:45.726526Z","iopub.execute_input":"2024-02-03T17:43:45.727122Z","iopub.status.idle":"2024-02-03T17:43:45.734362Z","shell.execute_reply.started":"2024-02-03T17:43:45.727096Z","shell.execute_reply":"2024-02-03T17:43:45.733400Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"batch_size = 4\n\ntrain_loader = DataLoader(train_dataset,shuffle= True , batch_size = batch_size)\nval_loader = DataLoader(val_dataset,shuffle = True , batch_size = batch_size)\ntest_loader = DataLoader(test_dataset,shuffle = True, batch_size = batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:43:45.735476Z","iopub.execute_input":"2024-02-03T17:43:45.735763Z","iopub.status.idle":"2024-02-03T17:43:45.743947Z","shell.execute_reply.started":"2024-02-03T17:43:45.735735Z","shell.execute_reply":"2024-02-03T17:43:45.743224Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"original_model= torchvision.models.vgg16(pretrained = True)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:44:03.974102Z","iopub.execute_input":"2024-02-03T17:44:03.974473Z","iopub.status.idle":"2024-02-03T17:44:05.452557Z","shell.execute_reply.started":"2024-02-03T17:44:03.974443Z","shell.execute_reply":"2024-02-03T17:44:05.451758Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class NewModel(nn.Module):\n    def __init__(self,model):\n        super(NewModel,self).__init__()\n        features = list(model.features.children())[1:-2]\n        for i in range(len(features)):\n            if isinstance(features[i], nn.Conv2d):\n                features.insert(i+1, nn.BatchNorm2d(features[i].out_channels))\n                \n        self.features = nn.Sequential(\n                        nn.Conv2d(1,64,kernel_size = 3,padding = 1,stride = 1),\n                        *features,\n                        nn.ReLU(inplace = True),\n                        nn.MaxPool2d(2))\n        \n        self.avgpool = nn.AvgPool2d(2)\n        \n        self.classifier = nn.Sequential(\n                            nn.Linear(512*3*3,512),\n                            nn.BatchNorm1d(512),\n                            nn.ReLU(inplace = True),\n                            nn.Dropout(p = 0.3,inplace = False),\n                            nn.Linear(512,5),\n                            )\n        \n            \n    def forward(self,x):\n            x = self.features(x)\n            x = self.avgpool(x)\n\n            x = x.view(x.shape[0],-1)\n            \n            x = self.classifier(x)\n            \n            return x","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:46:53.749304Z","iopub.execute_input":"2024-02-03T17:46:53.750026Z","iopub.status.idle":"2024-02-03T17:46:53.759746Z","shell.execute_reply.started":"2024-02-03T17:46:53.749996Z","shell.execute_reply":"2024-02-03T17:46:53.758833Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"new_model = NewModel(original_model)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:46:53.959341Z","iopub.execute_input":"2024-02-03T17:46:53.959659Z","iopub.status.idle":"2024-02-03T17:46:53.988740Z","shell.execute_reply.started":"2024-02-03T17:46:53.959636Z","shell.execute_reply":"2024-02-03T17:46:53.987685Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"new_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:46:54.128528Z","iopub.execute_input":"2024-02-03T17:46:54.129323Z","iopub.status.idle":"2024-02-03T17:46:54.135889Z","shell.execute_reply.started":"2024-02-03T17:46:54.129292Z","shell.execute_reply":"2024-02-03T17:46:54.134953Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"NewModel(\n  (features): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU(inplace=True)\n    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (15): ReLU(inplace=True)\n    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (18): ReLU(inplace=True)\n    (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (21): ReLU(inplace=True)\n    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (23): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (24): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (28): ReLU(inplace=True)\n    (29): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (30): ReLU(inplace=True)\n    (31): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (33): ReLU(inplace=True)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): ReLU(inplace=True)\n    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (37): ReLU(inplace=True)\n    (38): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  (classifier): Sequential(\n    (0): Linear(in_features=4608, out_features=512, bias=True)\n    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.3, inplace=False)\n    (4): Linear(in_features=512, out_features=5, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"filtered_state_dict = {k: v for k, v in original_model.state_dict().items() if k in new_model.state_dict() and v.shape == new_model.state_dict()[k].shape}","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:46:54.293633Z","iopub.execute_input":"2024-02-03T17:46:54.294374Z","iopub.status.idle":"2024-02-03T17:46:54.321203Z","shell.execute_reply.started":"2024-02-03T17:46:54.294343Z","shell.execute_reply":"2024-02-03T17:46:54.320496Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"new_model.load_state_dict(filtered_state_dict, strict=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:46:54.408575Z","iopub.execute_input":"2024-02-03T17:46:54.409303Z","iopub.status.idle":"2024-02-03T17:46:54.417404Z","shell.execute_reply.started":"2024-02-03T17:46:54.409271Z","shell.execute_reply":"2024-02-03T17:46:54.416482Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"_IncompatibleKeys(missing_keys=['features.0.weight', 'features.3.weight', 'features.3.bias', 'features.3.running_mean', 'features.3.running_var', 'features.6.weight', 'features.6.bias', 'features.7.weight', 'features.7.running_mean', 'features.7.running_var', 'features.9.weight', 'features.9.bias', 'features.10.weight', 'features.10.bias', 'features.10.running_mean', 'features.10.running_var', 'features.13.weight', 'features.13.bias', 'features.14.weight', 'features.14.running_mean', 'features.14.running_var', 'features.16.weight', 'features.16.bias', 'features.17.weight', 'features.17.bias', 'features.17.running_mean', 'features.17.running_var', 'features.19.weight', 'features.19.bias', 'features.20.weight', 'features.20.bias', 'features.20.running_mean', 'features.20.running_var', 'features.23.weight', 'features.23.bias', 'features.24.weight', 'features.24.running_mean', 'features.24.running_var', 'features.27.weight', 'features.27.bias', 'features.27.running_mean', 'features.27.running_var', 'features.29.weight', 'features.29.bias', 'features.32.weight', 'features.32.bias', 'features.34.weight', 'features.34.bias', 'features.36.weight', 'features.36.bias', 'classifier.0.weight', 'classifier.0.bias', 'classifier.1.weight', 'classifier.1.bias', 'classifier.1.running_mean', 'classifier.1.running_var', 'classifier.4.weight', 'classifier.4.bias'], unexpected_keys=[])"},"metadata":{}}]},{"cell_type":"code","source":"no_of_layers = 22\n\nfor i, params in enumerate(new_model.parameters()):\n    if i < no_of_layers:\n        params.requires_grad = False\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:46:54.568861Z","iopub.execute_input":"2024-02-03T17:46:54.569459Z","iopub.status.idle":"2024-02-03T17:46:54.574275Z","shell.execute_reply.started":"2024-02-03T17:46:54.569418Z","shell.execute_reply":"2024-02-03T17:46:54.573319Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"device = 'cuda'\nnew_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:46:54.724011Z","iopub.execute_input":"2024-02-03T17:46:54.724857Z","iopub.status.idle":"2024-02-03T17:46:54.736255Z","shell.execute_reply.started":"2024-02-03T17:46:54.724825Z","shell.execute_reply":"2024-02-03T17:46:54.735168Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"NewModel(\n  (features): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU(inplace=True)\n    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (15): ReLU(inplace=True)\n    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (18): ReLU(inplace=True)\n    (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (21): ReLU(inplace=True)\n    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (23): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (24): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (28): ReLU(inplace=True)\n    (29): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (30): ReLU(inplace=True)\n    (31): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (33): ReLU(inplace=True)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): ReLU(inplace=True)\n    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (37): ReLU(inplace=True)\n    (38): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  (classifier): Sequential(\n    (0): Linear(in_features=4608, out_features=512, bias=True)\n    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.3, inplace=False)\n    (4): Linear(in_features=512, out_features=5, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:46:54.883661Z","iopub.execute_input":"2024-02-03T17:46:54.884475Z","iopub.status.idle":"2024-02-03T17:46:54.888401Z","shell.execute_reply.started":"2024-02-03T17:46:54.884445Z","shell.execute_reply":"2024-02-03T17:46:54.887501Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(new_model.parameters(),lr = 0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:46:55.038448Z","iopub.execute_input":"2024-02-03T17:46:55.038771Z","iopub.status.idle":"2024-02-03T17:46:55.043618Z","shell.execute_reply.started":"2024-02-03T17:46:55.038742Z","shell.execute_reply":"2024-02-03T17:46:55.042704Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"training_loss = 0.0\ntraining_accuracy = 0\nvalidation_accuracy = 0\n\nfor epoch in range(8):\n    n_samples_train = 0\n    n_samples_val = 0\n    for i, (images, labels) in enumerate(train_loader):\n        \n        images = images.to(device)\n        labels = labels.to(device)\n        output = new_model(images)\n        loss = criterion(output, labels)\n        n_samples_train += labels.size(0)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        _, predicted = torch.max(output.data, 1)\n        training_accuracy += (predicted == labels).sum().item()\n\n        if (i + 1) % 64 == 0:\n            print(f'Training - Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {loss.item():.4f}')\n\n    print(f'Training Accuracy - Epoch: {epoch + 1}: {training_accuracy * 100 / n_samples_train:.2f}%')\n    training_accuracy = 0\n\n    with torch.no_grad():\n        for i, (val_images, val_labels) in enumerate(val_loader):\n\n            val_images = val_images.to(device)\n            val_labels = val_labels.to(device)\n            val_output = new_model(val_images)\n            _, val_predicted = torch.max(val_output.data, 1)\n            validation_accuracy += (val_predicted == val_labels).sum().item()\n            n_samples_val += val_labels.size(0)\n\n    print(f'Validation Accuracy - Epoch: {epoch + 1}: {validation_accuracy * 100 / n_samples_val:.2f}%')\n    validation_accuracy = 0\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T18:01:03.474545Z","iopub.execute_input":"2024-02-03T18:01:03.475482Z","iopub.status.idle":"2024-02-03T18:01:43.196744Z","shell.execute_reply.started":"2024-02-03T18:01:03.475447Z","shell.execute_reply":"2024-02-03T18:01:43.195779Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Training - Epoch: 1, Batch: 64, Loss: 0.6632\nTraining - Epoch: 1, Batch: 128, Loss: 0.7800\nTraining - Epoch: 1, Batch: 192, Loss: 0.5339\nTraining - Epoch: 1, Batch: 256, Loss: 0.5968\nTraining - Epoch: 1, Batch: 320, Loss: 0.4155\nTraining - Epoch: 1, Batch: 384, Loss: 1.5935\nTraining - Epoch: 1, Batch: 448, Loss: 0.6082\nTraining - Epoch: 1, Batch: 512, Loss: 1.0065\nTraining - Epoch: 1, Batch: 576, Loss: 0.3557\nTraining - Epoch: 1, Batch: 640, Loss: 0.1723\nTraining - Epoch: 1, Batch: 704, Loss: 0.5279\nTraining - Epoch: 1, Batch: 768, Loss: 1.0373\nTraining - Epoch: 1, Batch: 832, Loss: 0.3973\nTraining - Epoch: 1, Batch: 896, Loss: 0.5578\nTraining - Epoch: 1, Batch: 960, Loss: 0.9883\nTraining - Epoch: 1, Batch: 1024, Loss: 0.9830\nTraining - Epoch: 1, Batch: 1088, Loss: 0.6672\nTraining - Epoch: 1, Batch: 1152, Loss: 0.3153\nTraining - Epoch: 1, Batch: 1216, Loss: 0.4019\nTraining - Epoch: 1, Batch: 1280, Loss: 0.4422\nTraining - Epoch: 1, Batch: 1344, Loss: 0.2855\nTraining - Epoch: 1, Batch: 1408, Loss: 0.5616\nTraining Accuracy - Epoch: 1: 68.73%\nValidation Accuracy - Epoch: 1: 59.69%\n","output_type":"stream"}]},{"cell_type":"code","source":"with torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n    test_loss = 0.0\n    test_accuracy = 0\n    for i,(images, labels) in enumerate(test_loader):\n\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = new_model(images)\n        loss = criterion(outputs.data,labels)\n        _, predicted = torch.max(outputs.data, 1)\n        n_samples += labels.size(0)\n        n_correct += (predicted == labels).sum().item()\n        test_accuracy = n_correct\n        test_loss = loss.item()\n        if((i+1)%50 == 0):\n\n            test_accuracy = 0\n            test_loss = 0\n    acc = 100.0 * n_correct / n_samples\n    print(f'Accuracy of the network on the {n_samples} test images:{acc:.4f}%')","metadata":{"execution":{"iopub.status.busy":"2024-02-03T18:01:50.014465Z","iopub.execute_input":"2024-02-03T18:01:50.014815Z","iopub.status.idle":"2024-02-03T18:01:55.324913Z","shell.execute_reply.started":"2024-02-03T18:01:50.014791Z","shell.execute_reply":"2024-02-03T18:01:55.323964Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Accuracy of the network on the 1656 test images:61.9565%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}